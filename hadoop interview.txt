Responsible for implementation and ongoing administration of Hadoop infrastructure.
Aligning with the systems engineering team to propose and deploy new hardware and software environments required for Hadoop and to expand existing environments.
Working with data delivery teams to setup new Hadoop users. This job includes setting up Linux users, setting up Kerberos principals and testing HDFS, Hive, Pig and MapReduce access for the new users.
Cluster maintenance as well as creation and removal of nodes using tools like Ganglia, Nagios, Cloudera Manager Enterprise, Dell Open Manage and other tools.
Performance tuning of Hadoop clusters and Hadoop MapReduce routines.
Screen Hadoop cluster job performances and capacity planning
Monitor Hadoop cluster connectivity and security
Manage and review Hadoop log files.
File system management and monitoring.
HDFS support and maintenance.
Diligently teaming with the infrastructure, network, database, application and business intelligence teams to guarantee high data quality and availability.
Collaborating with application teams to install operating system and Hadoop updates, patches, version upgrades when required.
Point of Contact for Vendor escalation










HDFS

The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. 
HDFS has a master/slave architecture. 
An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. 
In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. 
HDFS exposes a file system namespace and allows user data to be stored in files.
 Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. 
The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. 
It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. 
The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.



MAP REDUCE 

MapReduce is a processing technique and a program model for distributed computing based on java
The MapReduce algorithm contains two important tasks, namely Map and Reduce
Map takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs). 
Secondly, reduce task, which takes the output from a map as an input and combines those data tuples into a smaller set of tuples. 
The major advantage of MapReduce is that it is easy to scale data processing over multiple computing nodes.


CAPACITY PLANNING
Picking a Distribution and Version of Hadoop
Master Hardware Selection
Operating System Selection and Preparation
Deployment Layout
Software
Hostnames, DNS, and Identification
Users, Groups, and Privileges
Disk Configuration
Choosing a Filesystem
Network Design



YARN

Apache Hadoop YARN is the resource management and job scheduling technology in the open source Hadoop distributed processing framework. 
One of Apache Hadoop's core components, YARN is responsible for allocating system resources to the various applications running in a 
Hadoop cluster and scheduling tasks to be executed on different cluster nodes.
YARN stands for Yet Another Resource Negotiator,
Apache Hadoop YARN decentralizes execution and monitoring of processing jobs by separating the various responsibilities into these components:

A global ResourceManager that accepts job submissions from users, schedules the jobs and allocates resources to them
A NodeManager slave that's installed at each node and functions as a monitoring and reporting agent of the ResourceManager
An ApplicationMaster that's created for each application to negotiate for resources and work with the NodeManager to execute and monitor tasks
Resource containers that are controlled by NodeManagers and assigned the system resources allocated to individual applications




 



As the sequence of the name MapReduce implies, the reduce task is always performed after the map job.